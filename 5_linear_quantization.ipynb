{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4d99413c",
   "metadata": {},
   "source": [
    "# 5 线性层量化\n",
    "在前面的过程中我们已经学习了对称量化和非对称量化，现在我们要尝试对模型中的线性层进行量化。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "540d60af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import copy\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import util.quant_tool as quant_tool"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3b80c5f",
   "metadata": {},
   "source": [
    "## 5.1 自定义量化模块"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c0c547e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义权重量化模块（per-channel 对称 int8 量化）\n",
    "class QuantLinear(nn.Module):\n",
    "    \"\"\"\n",
    "    简化版的权重量化线性层：\n",
    "    - 只量化 weight（symmetric per-channel int8）\n",
    "    - bias 保持 FP32\n",
    "    - 前向时：先临时反量化，再用 F.linear\n",
    "    \"\"\"\n",
    "    def __init__(self, in_features: int, out_features: int, bias: bool = True,dtype=torch.float32):\n",
    "        super().__init__()\n",
    "\n",
    "\n",
    "        # # qweight: int8，形状 [out_features, in_features]\n",
    "        self.register_buffer(\n",
    "            \"qweight\",\n",
    "            torch.empty(out_features, in_features, dtype=torch.int8),\n",
    "        )\n",
    "        # scale: per-output-channel，形状 [out_features, 1]\n",
    "        self.register_buffer(\n",
    "            \"scale\",\n",
    "            torch.ones(out_features, 1, dtype=dtype),\n",
    "        )\n",
    "        # 对称量化 zero_point 固定 0，这里留个占位方便扩展\n",
    "        self.register_buffer(\n",
    "            \"zero_point\",\n",
    "            torch.zeros(out_features, 1, dtype=dtype),\n",
    "        )\n",
    "        \n",
    "        if bias:\n",
    "            self.register_buffer(\"bias\", \n",
    "                                 torch.randn((1, out_features), \n",
    "                                             dtype=dtype))\n",
    "        else:\n",
    "            self.bias = None\n",
    "\n",
    "    @classmethod\n",
    "    def from_linear(cls, linear: nn.Linear, per_channel: bool=False, is_symmetric: bool=True,dtype=torch.float32,channel_dim=0) -> \"QuantLinear\":\n",
    "        \"\"\"\n",
    "        给定一个 nn.Linear，构造对应的 QuantLinear 并完成权重量化。\n",
    "        \"\"\"\n",
    "        qlinear = cls(\n",
    "            in_features=linear.in_features,\n",
    "            out_features=linear.out_features,\n",
    "            bias=linear.bias is not None,\n",
    "            dtype=dtype\n",
    "        )\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # 获取线性层的权重\n",
    "            weight = linear.weight\n",
    "            if is_symmetric:\n",
    "                # 对线性层进行对称量化\n",
    "                qparams = quant_tool.get_symmetric_qparams(weight,per_channel,channel_dim)\n",
    "                qweight = quant_tool.quantize_tensor(weight,qparams)\n",
    "            else:\n",
    "                # 对线性层进行非对称量化\n",
    "                qparams = quant_tool.get_asymmetric_qparams(weight,per_channel,channel_dim)\n",
    "                qweight = quant_tool.quantize_tensor(weight,qparams)\n",
    "\n",
    "            # 储存缩放信息\n",
    "            qlinear.qweight = qweight\n",
    "            qlinear.scale = qparams.scale\n",
    "            qlinear.zero_point = qparams.zero_point\n",
    "\n",
    "            # 对偏置项不做处理\n",
    "            if linear.bias is not None:\n",
    "                qlinear.bias = linear.bias\n",
    "        return qlinear\n",
    "\n",
    "    # 前向传播的时候需要进行反量化\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "\n",
    "        \n",
    "        # 反量化得到近似权重：w_hat = (q - z) * scale\n",
    "        # qweight: [out, in], scale: [out, 1]\n",
    "        w_hat = self.qweight.to(x.dtype)\n",
    "        output = F.linear(x, w_hat) * self.scale\n",
    "        if self.bias is not None:\n",
    "            output = output + self.bias\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1da7471b",
   "metadata": {},
   "source": [
    "## 5.2 定义模型量化入口\n",
    "输入一个模型，输出量化后的模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "70433679",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional, List\n",
    "# 对权重进行量化\n",
    "def quantize_model_weights(\n",
    "    model: nn.Module,\n",
    "    per_channel:bool=False,\n",
    "    is_symmetric:bool=True,\n",
    "    channel_dim:int=0,\n",
    "    modules_to_exclude: Optional[List[str]] = None,  # 可选参数，排除不需要量化的层\n",
    ") -> nn.Module:\n",
    "    \"\"\"\n",
    "    递归遍历模型，遇到 nn.Linear 就替换成 QuantLinear（权重量化）。\n",
    "    可以通过 modules_to_exclude 按模块名排除不想量化的层。\n",
    "    \"\"\"\n",
    "    if modules_to_exclude is None:\n",
    "        modules_to_exclude = []\n",
    "\n",
    "    for name, child in list(model.named_children()):\n",
    "        full_name = name\n",
    "\n",
    "        if isinstance(child, nn.Linear) and full_name not in modules_to_exclude:\n",
    "            setattr(model, name, QuantLinear.from_linear(child,per_channel=per_channel,is_symmetric=is_symmetric,dtype=child.weight.dtype,channel_dim=channel_dim))\n",
    "        else:\n",
    "            quantize_model_weights(child,per_channel,is_symmetric,modules_to_exclude=modules_to_exclude)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af414f92",
   "metadata": {},
   "source": [
    "## 5.3 对模型进行量化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5fdb3e2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义一个多层线性层用于验证\n",
    "class FourLayerModel(nn.Module):\n",
    "    def __init__(self, input_size=64):\n",
    "        super(FourLayerModel, self).__init__()\n",
    "        self.layer1 = nn.Linear(input_size, 128)\n",
    "        self.layer2 = nn.Linear(128, 256)\n",
    "        self.layer3 = nn.Linear(256, 128)\n",
    "        self.layer4 = nn.Linear(128, 1)\n",
    "        self.layer5 = nn.Linear(1, 9)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.layer1(x))  \n",
    "        x = torch.relu(self.layer2(x))  \n",
    "        x = torch.relu(self.layer3(x))  \n",
    "        x = torch.relu(self.layer4(x))  \n",
    "        x = self.layer5(x) \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c397fe63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FourLayerModel(\n",
       "  (layer1): Linear(in_features=64, out_features=128, bias=True)\n",
       "  (layer2): Linear(in_features=128, out_features=256, bias=True)\n",
       "  (layer3): Linear(in_features=256, out_features=128, bias=True)\n",
       "  (layer4): Linear(in_features=128, out_features=1, bias=True)\n",
       "  (layer5): Linear(in_features=1, out_features=9, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = FourLayerModel()\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "133b5004",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FourLayerModel(\n",
       "  (layer1): Linear(in_features=64, out_features=128, bias=True)\n",
       "  (layer2): QuantLinear()\n",
       "  (layer3): QuantLinear()\n",
       "  (layer4): QuantLinear()\n",
       "  (layer5): Linear(in_features=1, out_features=9, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 对模型进行量化,排除['layer1']层\n",
    "base_model = copy.deepcopy(model)\n",
    "q_model = quantize_model_weights(model=base_model,modules_to_exclude=['layer1','layer5'])\n",
    "q_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1d8dee09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FourLayerModel(\n",
       "  (layer1): QuantLinear()\n",
       "  (layer2): QuantLinear()\n",
       "  (layer3): QuantLinear()\n",
       "  (layer4): QuantLinear()\n",
       "  (layer5): QuantLinear()\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import copy\n",
    "base_model = copy.deepcopy(model)\n",
    "# 对模型进行量化,不排除层\n",
    "q_model = quantize_model_weights(base_model)\n",
    "q_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "27e6716b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.0460,  0.6507, -0.0024, -0.0910,  0.7244,  1.1987, -1.5149, -0.5180,\n",
       "         -0.0269,  0.0533,  0.1340, -1.1349, -2.2192,  0.2176, -0.2943, -1.0202,\n",
       "         -1.4823,  1.9782,  0.5639,  0.0028,  0.9610,  1.2390,  0.3270,  0.8854,\n",
       "          1.7788,  0.5284,  0.3881, -0.4657,  1.2609, -2.3039, -0.1030,  0.8527,\n",
       "         -0.4935, -0.4974,  0.3793, -0.8307,  1.7702,  0.3372,  0.7004,  1.6341,\n",
       "          0.8201, -0.5599, -0.5949,  0.5014, -0.7524, -0.1732, -1.8799, -0.8076,\n",
       "         -0.6814,  1.3539, -0.1717,  1.5322,  0.4549,  1.2054,  0.5036, -0.6115,\n",
       "          0.8582,  0.0821, -0.2673, -1.4161, -0.2485, -1.6907, -0.3361, -0.3935]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 初始化随机输入\n",
    "input_data = torch.randn(1,64)\n",
    "input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3598219c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.7181,  0.1854, -0.6549,  0.7976, -0.1812,  0.8611,  0.6819, -0.9689,\n",
       "          0.6257]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 模型量化前输出\n",
    "model.eval()\n",
    "# 禁用梯度计算，进行推理\n",
    "with torch.no_grad():\n",
    "    output = model(input_data)\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6df07445",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.7180,  0.1855, -0.6549,  0.7975, -0.1810,  0.8612,  0.6821, -0.9688,\n",
       "          0.6256]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 模型量化后输出\n",
    "q_model.eval()\n",
    "# 禁用梯度计算，进行推理\n",
    "with torch.no_grad():\n",
    "    sym_output = q_model(input_data)\n",
    "sym_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c9a7fe3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "err: \n",
      " tensor([[-0.0807, -0.1463, -0.0560,  0.1019, -0.1965, -0.0707, -0.2319, -0.0903,\n",
      "          0.0679]]) \n"
     ]
    }
   ],
   "source": [
    "# 计算误差，差值较小有时候会省略显示，通过 * 1000 放大误差用于展示\n",
    "err = output - sym_output\n",
    "print(f\"err: \\n {err * 1000} \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "209d0289",
   "metadata": {},
   "source": [
    "### 以上方法已被装入quant_tool可直接通过quant_tool进行调用"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "387dad68",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fquant",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
