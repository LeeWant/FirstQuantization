{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4d99413c",
   "metadata": {},
   "source": [
    "# 5 线性层量化\n",
    "在前面的过程中我们已经学习了对称量化和非对称量化，现在我们要尝试对模型中的线性层进行量化。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "540d60af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import util.quant_tool as quant_tool"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3b80c5f",
   "metadata": {},
   "source": [
    "## 5.1 自定义量化模块"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c0c547e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义权重量化模块（per-channel 对称 int8 量化）\n",
    "class QuantLinear(nn.Module):\n",
    "    \"\"\"\n",
    "    简化版的权重量化线性层：\n",
    "    - 只量化 weight（symmetric per-channel int8）\n",
    "    - bias 保持 FP32\n",
    "    - 前向时：先临时反量化，再用 F.linear\n",
    "    \"\"\"\n",
    "    def __init__(self, in_features: int, out_features: int, bias: bool = True):\n",
    "        super().__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "\n",
    "        # # qweight: int8，形状 [out_features, in_features]\n",
    "        self.register_buffer(\n",
    "            \"qweight\",\n",
    "            torch.empty(out_features, in_features, dtype=torch.int8),\n",
    "        )\n",
    "        # scale: per-output-channel，形状 [out_features, 1]\n",
    "        self.register_buffer(\n",
    "            \"scale\",\n",
    "            torch.ones(out_features, 1, dtype=torch.float32),\n",
    "        )\n",
    "        # 对称量化 zero_point 固定 0，这里留个占位方便扩展\n",
    "        self.register_buffer(\n",
    "            \"zero_point\",\n",
    "            torch.zeros(out_features, 1, dtype=torch.float32),\n",
    "        )\n",
    "        \n",
    "        if bias:\n",
    "            self.bias = nn.Parameter(torch.zeros(out_features, dtype=torch.float32))\n",
    "        else:\n",
    "            self.bias = None\n",
    "\n",
    "    @classmethod\n",
    "    def from_linear(cls, linear: nn.Linear, per_channel: bool=False, is_symmetric: bool=True) -> \"QuantLinear\":\n",
    "        \"\"\"\n",
    "        给定一个 nn.Linear，构造对应的 QuantLinear 并完成权重量化。\n",
    "        \"\"\"\n",
    "        qlinear = cls(\n",
    "            in_features=linear.in_features,\n",
    "            out_features=linear.out_features,\n",
    "            bias=linear.bias is not None,\n",
    "        )\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # 获取线性层的权重\n",
    "            weight = linear.weight.data.detach().float().cpu()\n",
    "\n",
    "            if is_symmetric:\n",
    "                # 对线性层进行对称量化\n",
    "                qparams = quant_tool.get_symmetric_qparams(weight,per_channel)\n",
    "                qweight = quant_tool.quantize_tensor(weight,qparams)\n",
    "            else:\n",
    "                # 对线性层进行非对称量化\n",
    "                qparams = quant_tool.get_asymmetric_qparams(weight,per_channel)\n",
    "                qweight = quant_tool.quantize_tensor(weight,qparams)\n",
    "\n",
    "            # 储存缩放信息\n",
    "            qlinear.qweight.copy_(qweight)\n",
    "            qlinear.scale.copy_(qparams.scale)\n",
    "            qlinear.zero_point.copy_(qparams.zero_point)\n",
    "\n",
    "            # 对偏置项不做处理\n",
    "            if linear.bias is not None:\n",
    "                qlinear.bias.data.copy_(linear.bias.data.detach().float().cpu())\n",
    "        return qlinear\n",
    "\n",
    "    # 前向传播的时候需要进行反量化\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # 反量化得到近似权重：w_hat = (q - z) * scale\n",
    "        # qweight: [out, in], scale: [out, 1]\n",
    "        w_hat = (self.qweight.float()) * self.scale\n",
    "        return F.linear(x, w_hat, self.bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1da7471b",
   "metadata": {},
   "source": [
    "## 5.2 定义模型量化入口\n",
    "输入一个模型，输出量化后的模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "70433679",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional, List\n",
    "# 对权重进行量化\n",
    "def quantize_model_weights(\n",
    "    model: nn.Module,\n",
    "    modules_to_exclude: Optional[List[str]] = None,  # 可选参数，排除不需要量化的层\n",
    ") -> nn.Module:\n",
    "    \"\"\"\n",
    "    递归遍历模型，遇到 nn.Linear 就替换成 QuantLinear（权重量化）。\n",
    "    可以通过 modules_to_exclude 按模块名排除不想量化的层。\n",
    "    \"\"\"\n",
    "    if modules_to_exclude is None:\n",
    "        modules_to_exclude = []\n",
    "\n",
    "    for name, child in list(model.named_children()):\n",
    "        full_name = name\n",
    "\n",
    "        if isinstance(child, nn.Linear) and full_name not in modules_to_exclude:\n",
    "            setattr(model, name, QuantLinear.from_linear(child))\n",
    "        else:\n",
    "            quantize_model_weights(child, modules_to_exclude=modules_to_exclude)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af414f92",
   "metadata": {},
   "source": [
    "## 5.3 对模型进行量化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5fdb3e2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义一个多层线性层用于验证\n",
    "class FourLayerModel(nn.Module):\n",
    "    def __init__(self, input_size=64, hidden_size1=64, hidden_size2=128, hidden_size3=128, output_size=256):\n",
    "        super(FourLayerModel, self).__init__()\n",
    "        self.layer1 = nn.Linear(input_size, hidden_size1)\n",
    "        self.layer2 = nn.Linear(hidden_size1, hidden_size2)\n",
    "        self.layer3 = nn.Linear(hidden_size2, hidden_size3)\n",
    "        self.layer4 = nn.Linear(hidden_size3, output_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.layer1(x))  \n",
    "        x = torch.relu(self.layer2(x))  \n",
    "        x = torch.relu(self.layer3(x))  \n",
    "        x = self.layer4(x) \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c397fe63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FourLayerModel(\n",
       "  (layer1): Linear(in_features=64, out_features=64, bias=True)\n",
       "  (layer2): Linear(in_features=64, out_features=128, bias=True)\n",
       "  (layer3): Linear(in_features=128, out_features=128, bias=True)\n",
       "  (layer4): Linear(in_features=128, out_features=256, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = FourLayerModel()\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1d8dee09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FourLayerModel(\n",
       "  (layer1): QuantLinear()\n",
       "  (layer2): QuantLinear()\n",
       "  (layer3): QuantLinear()\n",
       "  (layer4): QuantLinear()\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import copy\n",
    "base_model = copy.deepcopy(model)\n",
    "# 对模型进行量化,不排除层\n",
    "q_model = quantize_model_weights(base_model)\n",
    "q_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "946f478b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FourLayerModel(\n",
       "  (layer1): Linear(in_features=64, out_features=64, bias=True)\n",
       "  (layer2): QuantLinear()\n",
       "  (layer3): QuantLinear()\n",
       "  (layer4): QuantLinear()\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 对模型进行量化,排除['layer1']层\n",
    "base_model = copy.deepcopy(model)\n",
    "q_model = quantize_model_weights(model=base_model,modules_to_exclude=['layer1'])\n",
    "q_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7577dd82",
   "metadata": {},
   "source": [
    "## 5.3 加载模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1aea5be4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 安装transformers\n",
    "# !pip install transformers==4.35.0 accelerate==0.26.1 seaborn==0.13.1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9d475beb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45ec105a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c33a9e2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b4e15a0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fquant",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
