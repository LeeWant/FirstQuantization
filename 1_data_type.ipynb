{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4747d1e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install torch==2.1.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd878252",
   "metadata": {},
   "source": [
    "# 1.1 数据类型\n",
    "学习用于存储机器学习模型参数的常见数据类型。\n",
    "\n",
    "![不同数据类型的二进制表示过程](./img/1_binary_data_storage.png)\n",
    "\n",
    "<!-- <img src=\"./img/1_binary_data_storage.png\" alt=\"不同数据类型的二进制表示过程\" width=\"800\" /> -->\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "图片展示了计算机是如何使用二进制进行数据的存储，以32为浮点数（FP32）为例，计算机使用了32位二进制进行表示，其中符号位占了1位（首位），指数位占了8位，其余23位为小数位。\n",
    "\n",
    "对于一个10B的模型使用FP64进行存储的模型来说，其占用的静态存储空间约为74.5GB，因为每个参数占64/8个字节，10亿参数则占用了80亿字节，即74.5GB。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b62b931",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0eedc5a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9582cc55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "finfo(resolution=1e-06, min=-3.40282e+38, max=3.40282e+38, eps=1.19209e-07, smallest_normal=1.17549e-38, tiny=1.17549e-38, dtype=float32)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 使用finfo查看torch中浮点数的数据情况\n",
    "torch.finfo(torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ca62c2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "finfo(resolution=0.01, min=-3.38953e+38, max=3.38953e+38, eps=0.0078125, smallest_normal=1.17549e-38, tiny=1.17549e-38, dtype=bfloat16)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# bf16和pf32有一样的指数位宽，因此有相同的动态范围，但是bf16的精度（小数位的位宽）没有fp32多\n",
    "torch.finfo(torch.bfloat16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34a7bdc1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "finfo(resolution=0.001, min=-65504, max=65504, eps=0.000976562, smallest_normal=6.10352e-05, tiny=6.10352e-05, dtype=float16)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fp16的动态范围比bf16小，但是精度比bf16高（能表示更精确的浮点数）\n",
    "torch.finfo(torch.float16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bea28e47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "iinfo(min=-128, max=127, dtype=int8)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 使用iinfo查看int类型的情况\n",
    "torch.iinfo(torch.int8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "60c9f093",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "iinfo(min=0, max=255, dtype=uint8)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 无符号数\n",
    "torch.iinfo(torch.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a28c4e11",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'torch' has no attribute 'int4'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m torch\u001b[38;5;241m.\u001b[39miinfo(\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mint4\u001b[49m)\n",
      "File \u001b[0;32m~/.conda/envs/fquant/lib/python3.10/site-packages/torch/__init__.py:1833\u001b[0m, in \u001b[0;36m__getattr__\u001b[0;34m(name)\u001b[0m\n\u001b[1;32m   1830\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mimportlib\u001b[39;00m\n\u001b[1;32m   1831\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m importlib\u001b[38;5;241m.\u001b[39mimport_module(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;18m__name__\u001b[39m)\n\u001b[0;32m-> 1833\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodule \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'torch' has no attribute 'int4'"
     ]
    }
   ],
   "source": [
    "# torch中没有int4和int2的原生实现，因此会报错。\n",
    "# torch.iinfo(torch.int4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "853469c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义一个fp64位的小数\n",
    "v = 1/3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c2aead2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.333333333333333314829616256247390992939472198486328125000000'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 展示前60位\n",
    "format(v,'.60f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e3102959",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fp64 tensor: 0.333333333333333314829616256247390992939472198486328125000000\n"
     ]
    }
   ],
   "source": [
    "# 使用tensor存储64位浮点数\n",
    "tensor_fp64 = torch.tensor(v, dtype = torch.float64)\n",
    "print(f\"fp64 tensor: {format(tensor_fp64.item(), '.60f')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "df505f98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 分别转化为fp32\\fp16\\bf16\n",
    "tensor_fp32 = torch.tensor(v, dtype = torch.float32)\n",
    "tensor_fp16 = torch.tensor(v, dtype = torch.float16)\n",
    "tensor_bf16 = torch.tensor(v, dtype = torch.bfloat16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f10b88a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fp64 tensor: 0.333333333333333314829616256247390992939472198486328125000000\n",
      "fp32 tensor: 0.333333343267440795898437500000000000000000000000000000000000\n",
      "fp16 tensor: 0.333251953125000000000000000000000000000000000000000000000000\n",
      "bf16 tensor: 0.333984375000000000000000000000000000000000000000000000000000\n"
     ]
    }
   ],
   "source": [
    "print(f\"fp64 tensor: {format(tensor_fp64.item(), '.60f')}\")\n",
    "print(f\"fp32 tensor: {format(tensor_fp32.item(), '.60f')}\")\n",
    "print(f\"fp16 tensor: {format(tensor_fp16.item(), '.60f')}\")\n",
    "print(f\"bf16 tensor: {format(tensor_bf16.item(), '.60f')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef01e43b",
   "metadata": {},
   "source": [
    "# 1.2 量化的本质\n",
    "\n",
    "<img src=\"./img/1_essence.png\" alt=\"数据类型映射关系\" width=\"800\" />\n",
    "\n",
    "量化实际上就是集合之间的映射，需要找个一个映射方式尽量减少损失的情况下使用更小的集合表达更大的集合。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cbb2b3a",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fquant",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
